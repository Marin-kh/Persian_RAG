{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marin-kh/Persian_RAG/blob/main/Persian_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qVPZcCqTiMTH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4efd5cd-9ea1-4c6c-9b64-99a0c6beba79",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from hazm) (4.3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from hazm) (3.9.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.11/dist-packages (from hazm) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from hazm) (1.6.1)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.2.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.17.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Collecting rake_nltk\n",
            "  Using cached rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from rake_nltk) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.67.1)\n",
            "Using cached rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: rake_nltk\n",
            "Successfully installed rake_nltk-1.0.6\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.2.1)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=19fd58aef44869f42a89043111528fb9913956c020c4c47c579c91e22b9bd783\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "docx"
                ]
              },
              "id": "8a0d6f3907cc43e19bff8971c1eb5d41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (1.24.3)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/363.4 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "!pip install python-docx\n",
        "!pip install rake_nltk\n",
        "!pip install docx\n",
        "!pip install stanza\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hazm import stopwords_list, Normalizer, WordTokenizer, SentenceTokenizer, Stemmer, Lemmatizer, sent_tokenize, word_tokenize\n",
        "import docx\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "import nltk\n",
        "from rake_nltk import Rake\n",
        "from google.colab import drive\n",
        "import stanza\n",
        "from collections import defaultdict\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "T50h_ArvJ3Ia"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download('fa')\n",
        "nlp = stanza.Pipeline('fa')"
      ],
      "metadata": {
        "id": "AZWv4d3mp2Ta",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PersianRAKE(Rake):\n",
        "    def _tokenize_text_to_sentences(self, text: str):\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "    def _tokenize_sentence_to_words(self, sentence: str):\n",
        "        return word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "waRPUyQUJ5jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_from_docx(doc):\n",
        "    fullText=''\n",
        "    for pra in doc.paragraphs:\n",
        "        fullText+=pra.text+' '\n",
        "\n",
        "    return fullText\n",
        "\n",
        "def split_into_overlapping_chunks(sentences, max_chunk_size=1000, overlap_size=200):\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_chunk_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence)\n",
        "\n",
        "        if current_chunk_size + sentence_length > max_chunk_size and current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "            overlap_buffer = current_chunk[-overlap_size:].strip() if current_chunk else \"\"\n",
        "            current_chunk = overlap_buffer + \" \"\n",
        "            current_chunk_size = len(overlap_buffer) + 1\n",
        "\n",
        "        current_chunk += sentence + \" \"\n",
        "        current_chunk_size += sentence_length + 1\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def preprocess_text_1(text):\n",
        "    # text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'( +)', ' ', str(text))\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_text_2(text):\n",
        "    text = re.sub('(\\(.*?\\))|(\\[.*?\\])', '', str(text))\n",
        "    text = re.sub(r'( +)', ' ', str(text))\n",
        "\n",
        "    word_tokenizer = WordTokenizer()\n",
        "    words = word_tokenizer.tokenize(text)\n",
        "\n",
        "    stopwords = stopwords_list()\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "    lemmatizer = Lemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "\n",
        "def check_spelling(main_text):\n",
        "    endpoint = \"https://api.languagetool.org/v2/check\"\n",
        "\n",
        "    data = {\n",
        "        \"text\": main_text,\n",
        "        \"language\": \"en-US\",\n",
        "    }\n",
        "\n",
        "    response = requests.post(endpoint, data=data)\n",
        "    json_response = response.json()\n",
        "\n",
        "    updated_text = main_text\n",
        "\n",
        "    for match in json_response.get(\"matches\", []):\n",
        "        replacement = match[\"replacements\"][0][\"value\"] if match[\"replacements\"] else \"\"\n",
        "\n",
        "        offset = match[\"offset\"]\n",
        "        length = match[\"length\"]\n",
        "\n",
        "        updated_text = updated_text.replace(main_text[offset:offset+length], replacement)\n",
        "\n",
        "    print(\"Original Query: \", main_text)\n",
        "    print(\"Spell-checked Query: \", updated_text)\n",
        "    return updated_text\n",
        "\n",
        "def phrase_search(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    phrases = []\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            if word.upos in ['NOUN', 'ADJ']:\n",
        "                phrase = word.text\n",
        "                for other_word in sent.words:\n",
        "                    if other_word.head == word.id and other_word.upos in ['NOUN', 'ADJ']:\n",
        "                        phrase += \" \" + other_word.text\n",
        "                if \" \" in phrase:\n",
        "                    phrases.append(phrase)\n",
        "    return phrases\n",
        "\n",
        "def english_to_persian_number(number_str):\n",
        "    english_to_persian = {\n",
        "        \"0\": \"۰\",\n",
        "        \"1\": \"۱\",\n",
        "        \"2\": \"۲\",\n",
        "        \"3\": \"۳\",\n",
        "        \"4\": \"۴\",\n",
        "        \"5\": \"۵\",\n",
        "        \"6\": \"۶\",\n",
        "        \"7\": \"۷\",\n",
        "        \"8\": \"۸\",\n",
        "        \"9\": \"۹\",\n",
        "    }\n",
        "    persian_number = \"\".join([english_to_persian[digit] for digit in number_str])\n",
        "    return persian_number\n",
        "\n",
        "def persian_words_to_number(sentence):\n",
        "    word_to_number = {\n",
        "        \"صفر\": 0,\n",
        "        \"یک\": 1,\n",
        "        \"دو\": 2,\n",
        "        \"سه\": 3,\n",
        "        \"چهار\": 4,\n",
        "        \"پنج\": 5,\n",
        "        \"شش\": 6,\n",
        "        \"هفت\": 7,\n",
        "        \"هشت\": 8,\n",
        "        \"نه\": 9,\n",
        "        \"ده\": 10,\n",
        "        \"یازده\": 11,\n",
        "        \"دوازده\": 12,\n",
        "        \"سیزده\": 13,\n",
        "        \"چهارده\": 14,\n",
        "        \"پانزده\": 15,\n",
        "        \"شانزده\": 16,\n",
        "        \"هفده\": 17,\n",
        "        \"هجده\": 18,\n",
        "        \"نوزده\": 19,\n",
        "        \"بیست\": 20,\n",
        "        \"سی\": 30,\n",
        "        \"چهل\": 40,\n",
        "        \"پنجاه\": 50,\n",
        "        \"شصت\": 60,\n",
        "        \"هفتاد\": 70,\n",
        "        \"هشتاد\": 80,\n",
        "        \"نود\": 90,\n",
        "        \"صد\": 100,\n",
        "        \"یکصد\": 100,\n",
        "        \"دویست\": 200,\n",
        "        \"سیصد\": 300,\n",
        "        \"چهارصد\": 400,\n",
        "        \"پانصد\": 500,\n",
        "        \"ششصد\": 600,\n",
        "        \"هفتصد\": 700,\n",
        "        \"هشتصد\": 800,\n",
        "        \"نهصد\": 900,\n",
        "        \"هزار\": 1000,\n",
        "    }\n",
        "    words = sentence.split(' ')\n",
        "\n",
        "    result = []\n",
        "    temp_number_words = []\n",
        "    current_number = 0\n",
        "\n",
        "    for word in words:\n",
        "        if word[-1:]=='م' and (word[:-1] in word_to_number):\n",
        "            word = word[:-1]\n",
        "        if word in word_to_number:\n",
        "            temp_number_words.append(word)\n",
        "            current_number += word_to_number[word]\n",
        "        else:\n",
        "            if temp_number_words:\n",
        "                english_number_str = str(current_number)\n",
        "                persian_number_str = english_to_persian_number(english_number_str)\n",
        "                result.append(persian_number_str)\n",
        "                temp_number_words = []\n",
        "                current_number = 0\n",
        "            result.append(word)\n",
        "\n",
        "    if temp_number_words:\n",
        "        english_number_str = str(current_number)\n",
        "        persian_number_str = english_to_persian_number(english_number_str)\n",
        "        result.append(persian_number_str)\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def preprocess_phrases(text, phrases):\n",
        "    for phrase in phrases:\n",
        "        text = text.replace(phrase, phrase.replace(\" \", \"_\"))\n",
        "    return text\n",
        "\n",
        "def extract_persian_numbers(text):\n",
        "    persian_digits = \"۰۱۲۳۴۵۶۷۸۹\"\n",
        "    return re.findall(f\"[{persian_digits}]+\", text)\n",
        "\n",
        "def calculate_cosine_similarity(docs, phrase):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "    phrase_vector = vectorizer.transform(phrase)\n",
        "    return cosine_similarity(phrase_vector, tfidf_matrix)\n",
        "\n",
        "def calculate_tf(document_numbers):\n",
        "    tf = []\n",
        "    for doc in document_numbers:\n",
        "        tf_dict = defaultdict(int)\n",
        "        for num in doc:\n",
        "            tf_dict[num] += 1\n",
        "        tf.append(tf_dict)\n",
        "    return tf\n",
        "\n",
        "def calculate_idf(document_numbers, numbers):\n",
        "    idf = {}\n",
        "    total_docs = len(document_numbers)\n",
        "    for num in numbers:\n",
        "        doc_count = sum(1 for doc in document_numbers if num in doc)\n",
        "        idf[num] = np.log((total_docs + 1) / (doc_count + 1)) + 1\n",
        "    return idf\n",
        "\n",
        "def calculate_tf_idf(document_numbers, numbers):\n",
        "    tf = calculate_tf(document_numbers)\n",
        "    idf = calculate_idf(document_numbers, numbers)\n",
        "    tf_idf = []\n",
        "    for doc_tf in tf:\n",
        "        doc_tf_idf = {}\n",
        "        for num, freq in doc_tf.items():\n",
        "            if num in idf:\n",
        "                doc_tf_idf[num] = freq * idf[num]\n",
        "        tf_idf.append(doc_tf_idf)\n",
        "    return tf_idf"
      ],
      "metadata": {
        "id": "pDf4OEtAJ7n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading The Main Document\n",
        "drive.mount('/content/drive')\n",
        "# document = read_from_docx(docx.Document(\"/content/drive/My Drive/Constitution_of_the_Islamic_Republic.docx\"))\n",
        "document = read_from_docx(docx.Document(\"/content/drive/My Drive/delta.docx\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W91Qf-pzKAOC",
        "outputId": "ebaaf2f0-15b6-4015-c399-e31431bd4fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df = pd.read_csv('/content/drive/My Drive/api_key.csv')"
      ],
      "metadata": {
        "id": "oXIkVckY1rCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking The Document\n",
        "normalizer = Normalizer()\n",
        "normalized_text = normalizer.normalize(document)\n",
        "\n",
        "sentence_tokenizer = SentenceTokenizer()\n",
        "sentences = sentence_tokenizer.tokenize(normalized_text)\n",
        "\n",
        "max_chunk_size = 1000\n",
        "overlap_size = 200\n",
        "chunks = split_into_overlapping_chunks(sentences, max_chunk_size, overlap_size)\n",
        "print(\"<Chunk 1>\")\n",
        "print(f\"Original Chunk:\\n{chunks[0]}\")\n",
        "\n",
        "# Preprocessing The Chunks\n",
        "preprocessed1_chunks = [preprocess_text_1(chunk) for chunk in chunks]\n",
        "\n",
        "preprocessed2_chunks = [preprocess_text_2(chunk) for chunk in preprocessed1_chunks]\n",
        "print(f\"Preprocessed Chunk:\\n{preprocessed2_chunks[0]}\")"
      ],
      "metadata": {
        "id": "cNTikfkxKCzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4479b548-fcb1-4ec7-9579-e33380b26ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Chunk 1>\n",
            "Original Chunk:\n",
            "املاک دلتا یکی از شرکت‌های برجسته در صنعت املاک و مستغلات است که در زمینه خرید، فروش، اجاره و مدیریت املاک فعالیت می‌کند. این شرکت با بهره‌گیری از تجربه و تخصص خود در این حوزه، به مشتریان خود خدمات متنوعی ارائه می‌دهد. در این مقاله به بررسی تاریخچه، خدمات، مزایا و چالش‌های املاک دلتا خواهیم پرداخت: تاریخچه دلتا املاک دلتا در سال ۱۳۵۶ تأسیس شد و از آن زمان تا به امروز به یکی از نام‌های معتبر در صنعت املاک و مستغلات تبدیل‌شده است. این شرکت با هدف ارائه خدمات با کیفیت و ساختارهای نوآورانه به مشتریان، فعالیت خود را آغاز کرد. با گذشت زمان، املاک دلتا توانسته است با بهره‌گیری از تیمی متخصص و استفاده از فناوری‌های پیشرفته، جایگاه خود را در بازار مستحکم کند. خدمات املاک دلتا به ارائه طیف گسترده‌ای از خدمات در حوزه املاک و مستغلات می‌پردازد که شامل موارد زیر است: خرید و فروش املاک: این شرکت به مشتریان خود کمک می‌کند تا املاک مناسب برای خرید یا فروش را پیدا کنند. خدمات مشاوره‌ای در زمینه ارزیابی قیمت، معرفی املاک مناسب و فرآیندهای قانونی خرید و فروش از جمله خدمات این بخش است.\n",
            "Preprocessed Chunk:\n",
            "املاک دلتا شرکت برجسته صنعت املاک مستغلات زمینه خرید فروش اجاره مدیریت املاک فعالیت میکند شرکت بهرهگیری تجربه تخصص حوزه مشتریان خدمات متنوع ارائه میدهد مقاله بررسی تاریخچه خدمات مزایا چالش املاک دلتا پرداخت#پرداز تاریخچه دلتا املاک دلتا سال ۱۳۵۶ تأسیس زمان امروز نام معتبر صنعت املاک مستغلات تبدیلشده شرکت هدف ارائه خدمات کیفیت ساختار نوآورانه مشتریان فعالیت آغاز گذشت#گذر زمان املاک دلتا توانست#توان بهرهگیری تیمی متخصص استفاده فناوری پیشرفته جایگاه بازار مستحکم خدمات املاک دلتا ارائه طیف گستردهای خدمات حوزه املاک مستغلات میپردازد موارد خرید فروش املاک شرکت مشتریان کمک میکند املاک خرید فروش خدمات مشاور زمینه ارزیابی قیمت معرفی املاک فرآیند قانونی خرید فروش جمله خدمات\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_model(content):\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "        api_key=df.loc[2, 'api_key'],\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": content}],\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "def get_answer(query):\n",
        "    query = check_spelling(query)\n",
        "\n",
        "    processed_query = preprocess_text_2(preprocess_text_1(query))\n",
        "    query_n = persian_words_to_number(processed_query)\n",
        "    phrases = phrase_search(query_n)\n",
        "    numbers = re.findall(r'\\d+', query_n)\n",
        "    numberic_chunks = [persian_words_to_number(chunk) for chunk in preprocessed2_chunks]\n",
        "\n",
        "    # Similarity Of Query\n",
        "    query_bonus = calculate_cosine_similarity(numberic_chunks, [query_n]).flatten()\n",
        "\n",
        "    # Similarity Of Numbers\n",
        "    document_numbers = [extract_persian_numbers(doc) for doc in numberic_chunks]\n",
        "    tf_idf_numbers = calculate_tf_idf(document_numbers, numbers)\n",
        "\n",
        "    number_bonus = np.zeros(len(numberic_chunks))\n",
        "    for i, doc_tf_idf in enumerate(tf_idf_numbers):\n",
        "        for num in numbers:\n",
        "            if num in doc_tf_idf:\n",
        "                number_bonus[i] += doc_tf_idf[num]\n",
        "    if np.max(number_bonus) > 0:\n",
        "        number_bonus = number_bonus / np.max(number_bonus)\n",
        "\n",
        "    # Similarity Of Phrases\n",
        "    preprocessed_docs = [preprocess_phrases(doc, phrases) for doc in numberic_chunks]\n",
        "    preprocessed_phrases = preprocess_phrases(query_n, phrases)\n",
        "\n",
        "    phrases_similarity = calculate_cosine_similarity(preprocessed_docs, [preprocessed_phrases])\n",
        "    phrases_bonus = np.max(phrases_similarity, axis=0)\n",
        "    if np.max(phrases_bonus) > 0:\n",
        "        phrases_bonus = phrases_bonus / np.max(phrases_bonus)\n",
        "\n",
        "    query_coef = 0.5\n",
        "    numbers_coef = 0.3\n",
        "    phrases_coef = 0.2\n",
        "\n",
        "    hybrid_scores = (query_bonus * query_coef) + (number_bonus * numbers_coef) + (phrases_bonus * phrases_coef)\n",
        "\n",
        "    top_k = 3\n",
        "    indices = np.argsort(-hybrid_scores)[:top_k]\n",
        "\n",
        "    top_3 = [chunks[idx] for idx in indices]\n",
        "\n",
        "    total_score = 0\n",
        "    print(\"Top 3 Hybrid Results:\")\n",
        "    for idx, chunk in zip(indices, top_3):\n",
        "        print(f\"Chunk {idx + 1} (Score: {hybrid_scores[idx]:.2f}):\\n{chunk}\")\n",
        "        total_score += hybrid_scores[idx]\n",
        "\n",
        "\n",
        "    # making answer\n",
        "    if total_score <= 0.1:\n",
        "        prop_related = ask_model(f\"ببین این جمله داخل <> درباره ملک هست یا نه.<{query}> و فقد با یک کلمه جواب منو بده بله یا خیر.\")\n",
        "        if \"بله\" in prop_related:   #prop related\n",
        "            answer = 'متأسفانه، اطلاعاتی در این باره در دسترس نیست و نمی‌توانم به سوال شما پاسخ دهم. برای اطلاعات بیشتر با شماره پشتیبانی سایت دلتا(8686-021) تماس حاصل فرمایید.'\n",
        "        else:                       #none prop related\n",
        "            answer = 'متأسفانه، اطلاعاتی در این باره در دسترس نیست و نمی‌توانم به سوال شما پاسخ دهم.'\n",
        "    else:\n",
        "        model_content = f\"{top_3[0]}\\n{top_3[1]}\\n{top_3[2]}\\nطبق متن های بالا به طور خلاصه(در حد یک پاراگراف) به این سوال جواب بده و اشاره ای به کلمه پاراگراف یا متن نکن و نگو از متن داری استفاده میکنی و اگه جواب تو متن نبود بگو نمیتوانم جواب شما را بدهم:{query}\\n\"\n",
        "        answer = ask_model(model_content)\n",
        "    return [answer, total_score]"
      ],
      "metadata": {
        "id": "ytsgM8ejdPdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "input = gr.Textbox(label=\"سوال\")\n",
        "\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"پاسخ\"),\n",
        "    gr.Textbox(label=\"score=\")\n",
        "]\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=get_answer,\n",
        "    inputs=input,\n",
        "    outputs=outputs,\n",
        "    title=\"Chat Bot\",\n",
        "    allow_flagging=\"never\",\n",
        "    theme=\"dark\"\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "Lz9GGR2rkhyJ",
        "outputId": "fcbf226b-9d94-4938-b5be-e07bb3041452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1114: UserWarning: Cannot load dark. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/dark (Request ID: Root=1-6804d009-74b06cbf3404cc4e0c958abe;ab86da61-dda1-41b9-9d89-19c7aa698f8f)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://564f3f822e9a9e160b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://564f3f822e9a9e160b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}